{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  2017-10-20  Linear and logistic regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this lab is to explore linear and logistic regression, implement them yourself and learn to use their respective scikit-learn implementation.\n",
    "\n",
    "Let us start by loading some of the usual librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing, metrics\n",
    "import sys\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. (Vanilla) Gradient descent\n",
    "To optimise our error while doing linear regression, we will use gradient descent. \n",
    "\n",
    "__Reminder about gradient descent:__ This algorithm is commonly used to optimise convex functions. The idea is very simple â€” to reach the global optimum of a convex function from any point, we need to move in the direction opposite to that of greatest increase of the function. As the function is convex, this strategy will always take us to the global optimum. \n",
    "\n",
    "Now, the direction of greatest increase of the function is determined by taking the partial derivatives of the function with respect to every variable. For example, let us say we wish to optimise a convex function $f(x)$, where $x = (x_1, x_2, \\ldots, x_d)^T$. Let us say, we start at a point $s \\in \\mathbb{R}^d$. Then, the direction of greatest increase of $f$ at $s$ is given by \n",
    "\n",
    "$$\n",
    "\\nabla f(s) = \\left(\\frac{\\partial f(s)}{\\partial x_1}, \\frac{\\partial f(s)}{\\partial x_2}, \\ldots ,\\frac{\\partial f(s)}{\\partial x_d} \\right)^T.\n",
    "$$\n",
    "\n",
    "With this derivative, we design an update rule, which asks us to move in the direction opposite to the direction of greatest increase. By repeatedly applying this rule, we hope to reach the global minimum. We hence move to $t$, which is given by\n",
    "\n",
    "$$\n",
    "t = s - \\gamma \\nabla f(s),\n",
    "$$\n",
    "where $\\gamma$ is a parameter called the *learning rate*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now construct a class for the gradient descent optimiser. This is a generic class that tries to optimise any function that is given to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall use the power of Python to declare our class. Python supports *first-class functions*, which allows us to pass functions to and return function from other functions. We shall use this to define the gradient descent optimizer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question:__ Fill in the `GradientDescentOptimizer` class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GradientDescentOptimizer():\n",
    "    \"\"\" Class for optimization by gradient descent.\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "    f: function\n",
    "        the function to optimize\n",
    "    fprime: function \n",
    "        the gradient of the function to optimize\n",
    "    beta: np.array\n",
    "        the point where the function is currently being evaluated\n",
    "    lr: float\n",
    "        the learning rate\n",
    "    fx: float\n",
    "        the current function value\n",
    "    fgx: np.array\n",
    "        the current gradient value\n",
    "    beta_history: list of np.array\n",
    "        the list of all points where f has been evaluated\n",
    "    f_history: list of float\n",
    "        the list of all evaluated function values (corresponding to the points in beta_history)\n",
    "    \"\"\"\n",
    "    def __init__(self, f, fprime, start, lr=1e-1):\n",
    "        \"\"\"     \n",
    "        Parameters:\n",
    "        -----------\n",
    "        f: function\n",
    "            the function to optimize \n",
    "            Yes, we can pass functions as parameters! To call f within the code, use f followed \n",
    "            by its arguments enclosed between parentheses, as you would normally do: f(x)\n",
    "        fprime: function\n",
    "            the function's gradient\n",
    "        start: np.array\n",
    "            the starting point, at which we begin our search\n",
    "        lr: float\n",
    "            the learning rate\n",
    "        \"\"\"\n",
    "        # Store the parameters as attributes\n",
    "        self.f      = f\n",
    "        self.fprime = fprime\n",
    "        self.beta   = start\n",
    "        self.lr     = lr\n",
    "        # Save history as attributes\n",
    "        self.beta_history = [start]\n",
    "        \n",
    "    \n",
    "    def compute_fprime(self):\n",
    "        \"\"\" Compute the value of the gradient of f for our current point. \n",
    "        Update self.fgx accordingly.\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        self.fgx=self.fprime(self.beta)\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\" Take a gradient descent step. \n",
    "        Upgrade self.beta accordingly. \n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        self.compute_fprime()\n",
    "        self.beta-=self.lr*self.fgx\n",
    "\n",
    "        \n",
    "    def optimize(self, max_iter=100):\n",
    "        \"\"\"Use the gradient descent optimiser to optimise f.\n",
    "        Update self.f_history and self.beta_history accordingly.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        max_iter: int\n",
    "            Maximum number of iterations.        \n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        it=0\n",
    "        while it<max_iter:\n",
    "            self.step()\n",
    "            it +=1\n",
    "            self.beta_history+=[self.beta]\n",
    "    def print_result(self):\n",
    "        \"\"\" Print out result once optimization is complete.        \n",
    "        \"\"\"\n",
    "        sys.stdout.write(\" === Result ===\\n\")\n",
    "        sys.stdout.write(\"Best beta found: \" + str(self.beta) +'\\n')\n",
    "        sys.stdout.write(\"f(best beta) = \" + str(self.f(self.beta)) + '\\n')\n",
    "        sys.stdout.write(\"f\\'(best beta) = \" + str(self.fprime(self.beta)) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to optimise a simple function with this optimizer. Define a function $f(x) = \\left(x - \\begin{pmatrix}5 \\\\ 4\\end{pmatrix}\\right)^2$. Note that the input to $f$ is a vector of size 2. \n",
    "#(x-(5 4)).dot(x-(5 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question:__ Fill in the functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    # TODO\n",
    "    return np.sum((x-np.array([5,4]))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define another function which is the gradient of $f$ at a point $x$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fprime(x):\n",
    "    # TODO\n",
    "    return 2*(x-np.array([5,4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check that f and fprime do what you want them to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "[ 0 -8]\n"
     ]
    }
   ],
   "source": [
    "print(f(np.array([5,0])))\n",
    "print(fprime(np.array([5,0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now initialise a `GradientDescentOptimizer` using this function and its gradient. Run the optimise by calling `.optimize()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " === Result ===\n",
      "Best beta found: [ 5.  4.]\n",
      "f(best beta) = 1.55632745102e-18\n",
      "f'(best beta) = [ -1.74552817e-09  -1.78281834e-09]\n"
     ]
    }
   ],
   "source": [
    "gd = GradientDescentOptimizer(f, fprime, start=np.random.normal(size=(2,), loc=0.0, scale=1.0),lr=1e-1)\n",
    "# TODO\n",
    "gd.optimize()\n",
    "gd.print_result()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the progression of the value of `f` and see how it goes.\n",
    "Plot also the norm of the gradient at each point (Hint: use the `beta_history` parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x190a6eb8>"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEDCAYAAAAVyO4LAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x191a1f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "f, = plt.plot([gd.f(x) for x in gd.beta_history], 'b')  # TODO \n",
    "g, = plt.plot(np.array([np.sum(gd.fprime(x)**2)for x in gd.beta_history]),'g') # TODO\n",
    "plt.legend([f, g], ['f(x)', '||f\\'(x)||**2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear regression\n",
    "\n",
    "We will now implement a linear regression, first using the closed form solution, and second with our gradient descent.\n",
    "\n",
    "## 1.1 Linear regression data\n",
    "\n",
    "Our first data set regards the quality ratings of a white _vinho verde_. Each wine is described by a number of physico-chemical descriptors such as acidity, sulfur dioxide content, density or pH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.70</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.90</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.50</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.50</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.90</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.16</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.045</td>\n",
       "      <td>30.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>0.9949</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.47</td>\n",
       "      <td>9.6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.70</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.044</td>\n",
       "      <td>28.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>0.9938</td>\n",
       "      <td>3.22</td>\n",
       "      <td>0.45</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.033</td>\n",
       "      <td>11.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.56</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8.6</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.40</td>\n",
       "      <td>4.20</td>\n",
       "      <td>0.035</td>\n",
       "      <td>17.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>0.9947</td>\n",
       "      <td>3.14</td>\n",
       "      <td>0.53</td>\n",
       "      <td>9.7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7.9</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.040</td>\n",
       "      <td>16.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.9920</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.63</td>\n",
       "      <td>10.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.044</td>\n",
       "      <td>48.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.9912</td>\n",
       "      <td>3.54</td>\n",
       "      <td>0.52</td>\n",
       "      <td>12.4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8.3</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.62</td>\n",
       "      <td>19.25</td>\n",
       "      <td>0.040</td>\n",
       "      <td>41.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>1.0002</td>\n",
       "      <td>2.98</td>\n",
       "      <td>0.67</td>\n",
       "      <td>9.7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.032</td>\n",
       "      <td>28.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0.9914</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.55</td>\n",
       "      <td>11.4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.046</td>\n",
       "      <td>30.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.9928</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.36</td>\n",
       "      <td>9.6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.029</td>\n",
       "      <td>29.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.9892</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.39</td>\n",
       "      <td>12.8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.033</td>\n",
       "      <td>17.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>0.9917</td>\n",
       "      <td>3.12</td>\n",
       "      <td>0.53</td>\n",
       "      <td>11.3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.14</td>\n",
       "      <td>7.50</td>\n",
       "      <td>0.044</td>\n",
       "      <td>34.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.9955</td>\n",
       "      <td>3.22</td>\n",
       "      <td>0.50</td>\n",
       "      <td>9.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.029</td>\n",
       "      <td>29.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.9892</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.39</td>\n",
       "      <td>12.8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6.4</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.38</td>\n",
       "      <td>2.90</td>\n",
       "      <td>0.038</td>\n",
       "      <td>19.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.9912</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.35</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.049</td>\n",
       "      <td>41.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>3.47</td>\n",
       "      <td>0.48</td>\n",
       "      <td>10.5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>7.6</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.074</td>\n",
       "      <td>25.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.9937</td>\n",
       "      <td>3.05</td>\n",
       "      <td>0.51</td>\n",
       "      <td>9.3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.052</td>\n",
       "      <td>16.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.47</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.32</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.046</td>\n",
       "      <td>56.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>0.9955</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>10.4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>6.9</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.052</td>\n",
       "      <td>35.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.39</td>\n",
       "      <td>8.70</td>\n",
       "      <td>0.051</td>\n",
       "      <td>32.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>0.9961</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.53</td>\n",
       "      <td>10.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.047</td>\n",
       "      <td>17.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9914</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.49</td>\n",
       "      <td>11.6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.36</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.033</td>\n",
       "      <td>37.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.9906</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.71</td>\n",
       "      <td>12.3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.43</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.056</td>\n",
       "      <td>48.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.51</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.23</td>\n",
       "      <td>4.60</td>\n",
       "      <td>0.061</td>\n",
       "      <td>50.5</td>\n",
       "      <td>238.5</td>\n",
       "      <td>0.9958</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.60</td>\n",
       "      <td>9.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.28</td>\n",
       "      <td>4.80</td>\n",
       "      <td>0.063</td>\n",
       "      <td>31.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>0.9964</td>\n",
       "      <td>3.69</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>8.6</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.054</td>\n",
       "      <td>9.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.9941</td>\n",
       "      <td>2.95</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>6.7</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.31</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.046</td>\n",
       "      <td>30.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.64</td>\n",
       "      <td>10.7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.29</td>\n",
       "      <td>10.10</td>\n",
       "      <td>0.050</td>\n",
       "      <td>21.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>3.13</td>\n",
       "      <td>0.35</td>\n",
       "      <td>9.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>7.1</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.043</td>\n",
       "      <td>31.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.9898</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.37</td>\n",
       "      <td>12.7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.042</td>\n",
       "      <td>20.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0.9912</td>\n",
       "      <td>3.31</td>\n",
       "      <td>0.65</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.30</td>\n",
       "      <td>8.80</td>\n",
       "      <td>0.064</td>\n",
       "      <td>26.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.9961</td>\n",
       "      <td>2.94</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>6.7</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.28</td>\n",
       "      <td>5.40</td>\n",
       "      <td>0.060</td>\n",
       "      <td>21.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0.9949</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.37</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.053</td>\n",
       "      <td>34.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.77</td>\n",
       "      <td>10.6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>7.1</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.34</td>\n",
       "      <td>16.00</td>\n",
       "      <td>0.050</td>\n",
       "      <td>51.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>0.9985</td>\n",
       "      <td>3.21</td>\n",
       "      <td>0.60</td>\n",
       "      <td>9.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>7.1</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.20</td>\n",
       "      <td>6.10</td>\n",
       "      <td>0.063</td>\n",
       "      <td>47.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>0.9946</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.42</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>7.3</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.30</td>\n",
       "      <td>8.20</td>\n",
       "      <td>0.047</td>\n",
       "      <td>42.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>0.9966</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>7.1</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.61</td>\n",
       "      <td>11.80</td>\n",
       "      <td>0.045</td>\n",
       "      <td>54.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>0.9974</td>\n",
       "      <td>3.11</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>7.1</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.62</td>\n",
       "      <td>11.80</td>\n",
       "      <td>0.044</td>\n",
       "      <td>52.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>0.9975</td>\n",
       "      <td>3.12</td>\n",
       "      <td>0.46</td>\n",
       "      <td>8.7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.63</td>\n",
       "      <td>11.00</td>\n",
       "      <td>0.044</td>\n",
       "      <td>55.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>0.9974</td>\n",
       "      <td>3.09</td>\n",
       "      <td>0.44</td>\n",
       "      <td>8.7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.31</td>\n",
       "      <td>13.30</td>\n",
       "      <td>0.050</td>\n",
       "      <td>69.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>0.9972</td>\n",
       "      <td>3.22</td>\n",
       "      <td>0.48</td>\n",
       "      <td>9.7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>7.1</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.61</td>\n",
       "      <td>11.80</td>\n",
       "      <td>0.045</td>\n",
       "      <td>54.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>0.9974</td>\n",
       "      <td>3.11</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>7.1</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.62</td>\n",
       "      <td>11.80</td>\n",
       "      <td>0.044</td>\n",
       "      <td>52.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>0.9975</td>\n",
       "      <td>3.12</td>\n",
       "      <td>0.46</td>\n",
       "      <td>8.7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.63</td>\n",
       "      <td>11.00</td>\n",
       "      <td>0.044</td>\n",
       "      <td>55.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>0.9974</td>\n",
       "      <td>3.09</td>\n",
       "      <td>0.44</td>\n",
       "      <td>8.7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>6.1</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.43</td>\n",
       "      <td>7.50</td>\n",
       "      <td>0.049</td>\n",
       "      <td>65.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>0.9957</td>\n",
       "      <td>3.12</td>\n",
       "      <td>0.47</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>6.9</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.035</td>\n",
       "      <td>47.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>0.9900</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.40</td>\n",
       "      <td>12.6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>6.9</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.034</td>\n",
       "      <td>48.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>0.9899</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.41</td>\n",
       "      <td>12.6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>7.5</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.040</td>\n",
       "      <td>51.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.9916</td>\n",
       "      <td>3.21</td>\n",
       "      <td>0.44</td>\n",
       "      <td>11.5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>7.1</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.29</td>\n",
       "      <td>12.40</td>\n",
       "      <td>0.044</td>\n",
       "      <td>62.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0.9969</td>\n",
       "      <td>3.04</td>\n",
       "      <td>0.42</td>\n",
       "      <td>9.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.66</td>\n",
       "      <td>15.90</td>\n",
       "      <td>0.046</td>\n",
       "      <td>26.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>3.14</td>\n",
       "      <td>0.50</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>8.6</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.034</td>\n",
       "      <td>15.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>2.95</td>\n",
       "      <td>0.36</td>\n",
       "      <td>11.4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>9.8</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.46</td>\n",
       "      <td>10.50</td>\n",
       "      <td>0.038</td>\n",
       "      <td>4.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>2.89</td>\n",
       "      <td>0.30</td>\n",
       "      <td>10.1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.66</td>\n",
       "      <td>15.90</td>\n",
       "      <td>0.046</td>\n",
       "      <td>26.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>3.14</td>\n",
       "      <td>0.50</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0             7.0             0.270         0.36           20.70      0.045   \n",
       "1             6.3             0.300         0.34            1.60      0.049   \n",
       "2             8.1             0.280         0.40            6.90      0.050   \n",
       "3             7.2             0.230         0.32            8.50      0.058   \n",
       "4             7.2             0.230         0.32            8.50      0.058   \n",
       "5             8.1             0.280         0.40            6.90      0.050   \n",
       "6             6.2             0.320         0.16            7.00      0.045   \n",
       "7             7.0             0.270         0.36           20.70      0.045   \n",
       "8             6.3             0.300         0.34            1.60      0.049   \n",
       "9             8.1             0.220         0.43            1.50      0.044   \n",
       "10            8.1             0.270         0.41            1.45      0.033   \n",
       "11            8.6             0.230         0.40            4.20      0.035   \n",
       "12            7.9             0.180         0.37            1.20      0.040   \n",
       "13            6.6             0.160         0.40            1.50      0.044   \n",
       "14            8.3             0.420         0.62           19.25      0.040   \n",
       "15            6.6             0.170         0.38            1.50      0.032   \n",
       "16            6.3             0.480         0.04            1.10      0.046   \n",
       "17            6.2             0.660         0.48            1.20      0.029   \n",
       "18            7.4             0.340         0.42            1.10      0.033   \n",
       "19            6.5             0.310         0.14            7.50      0.044   \n",
       "20            6.2             0.660         0.48            1.20      0.029   \n",
       "21            6.4             0.310         0.38            2.90      0.038   \n",
       "22            6.8             0.260         0.42            1.70      0.049   \n",
       "23            7.6             0.670         0.14            1.50      0.074   \n",
       "24            6.6             0.270         0.41            1.30      0.052   \n",
       "25            7.0             0.250         0.32            9.00      0.046   \n",
       "26            6.9             0.240         0.35            1.00      0.052   \n",
       "27            7.0             0.280         0.39            8.70      0.051   \n",
       "28            7.4             0.270         0.48            1.10      0.047   \n",
       "29            7.2             0.320         0.36            2.00      0.033   \n",
       "..            ...               ...          ...             ...        ...   \n",
       "70            6.2             0.270         0.43            7.80      0.056   \n",
       "71            6.8             0.300         0.23            4.60      0.061   \n",
       "72            6.0             0.270         0.28            4.80      0.063   \n",
       "73            8.6             0.230         0.46            1.00      0.054   \n",
       "74            6.7             0.230         0.31            2.10      0.046   \n",
       "75            7.4             0.240         0.29           10.10      0.050   \n",
       "76            7.1             0.180         0.36            1.40      0.043   \n",
       "77            7.0             0.320         0.34            1.30      0.042   \n",
       "78            7.4             0.180         0.30            8.80      0.064   \n",
       "79            6.7             0.540         0.28            5.40      0.060   \n",
       "80            6.8             0.220         0.31            1.40      0.053   \n",
       "81            7.1             0.200         0.34           16.00      0.050   \n",
       "82            7.1             0.340         0.20            6.10      0.063   \n",
       "83            7.3             0.220         0.30            8.20      0.047   \n",
       "84            7.1             0.430         0.61           11.80      0.045   \n",
       "85            7.1             0.440         0.62           11.80      0.044   \n",
       "86            7.2             0.390         0.63           11.00      0.044   \n",
       "87            6.8             0.250         0.31           13.30      0.050   \n",
       "88            7.1             0.430         0.61           11.80      0.045   \n",
       "89            7.1             0.440         0.62           11.80      0.044   \n",
       "90            7.2             0.390         0.63           11.00      0.044   \n",
       "91            6.1             0.270         0.43            7.50      0.049   \n",
       "92            6.9             0.240         0.33            1.70      0.035   \n",
       "93            6.9             0.210         0.33            1.80      0.034   \n",
       "94            7.5             0.170         0.32            1.70      0.040   \n",
       "95            7.1             0.260         0.29           12.40      0.044   \n",
       "96            6.0             0.340         0.66           15.90      0.046   \n",
       "97            8.6             0.265         0.36            1.20      0.034   \n",
       "98            9.8             0.360         0.46           10.50      0.038   \n",
       "99            6.0             0.340         0.66           15.90      0.046   \n",
       "\n",
       "    free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                  45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                  14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                  30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                  47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                  47.0                 186.0   0.9956  3.19       0.40   \n",
       "5                  30.0                  97.0   0.9951  3.26       0.44   \n",
       "6                  30.0                 136.0   0.9949  3.18       0.47   \n",
       "7                  45.0                 170.0   1.0010  3.00       0.45   \n",
       "8                  14.0                 132.0   0.9940  3.30       0.49   \n",
       "9                  28.0                 129.0   0.9938  3.22       0.45   \n",
       "10                 11.0                  63.0   0.9908  2.99       0.56   \n",
       "11                 17.0                 109.0   0.9947  3.14       0.53   \n",
       "12                 16.0                  75.0   0.9920  3.18       0.63   \n",
       "13                 48.0                 143.0   0.9912  3.54       0.52   \n",
       "14                 41.0                 172.0   1.0002  2.98       0.67   \n",
       "15                 28.0                 112.0   0.9914  3.25       0.55   \n",
       "16                 30.0                  99.0   0.9928  3.24       0.36   \n",
       "17                 29.0                  75.0   0.9892  3.33       0.39   \n",
       "18                 17.0                 171.0   0.9917  3.12       0.53   \n",
       "19                 34.0                 133.0   0.9955  3.22       0.50   \n",
       "20                 29.0                  75.0   0.9892  3.33       0.39   \n",
       "21                 19.0                 102.0   0.9912  3.17       0.35   \n",
       "22                 41.0                 122.0   0.9930  3.47       0.48   \n",
       "23                 25.0                 168.0   0.9937  3.05       0.51   \n",
       "24                 16.0                 142.0   0.9951  3.42       0.47   \n",
       "25                 56.0                 245.0   0.9955  3.25       0.50   \n",
       "26                 35.0                 146.0   0.9930  3.45       0.44   \n",
       "27                 32.0                 141.0   0.9961  3.38       0.53   \n",
       "28                 17.0                 132.0   0.9914  3.19       0.49   \n",
       "29                 37.0                 114.0   0.9906  3.10       0.71   \n",
       "..                  ...                   ...      ...   ...        ...   \n",
       "70                 48.0                 244.0   0.9956  3.10       0.51   \n",
       "71                 50.5                 238.5   0.9958  3.32       0.60   \n",
       "72                 31.0                 201.0   0.9964  3.69       0.71   \n",
       "73                  9.0                  72.0   0.9941  2.95       0.49   \n",
       "74                 30.0                  96.0   0.9926  3.33       0.64   \n",
       "75                 21.0                 105.0   0.9962  3.13       0.35   \n",
       "76                 31.0                  87.0   0.9898  3.26       0.37   \n",
       "77                 20.0                  69.0   0.9912  3.31       0.65   \n",
       "78                 26.0                 103.0   0.9961  2.94       0.56   \n",
       "79                 21.0                 105.0   0.9949  3.27       0.37   \n",
       "80                 34.0                 114.0   0.9929  3.39       0.77   \n",
       "81                 51.0                 166.0   0.9985  3.21       0.60   \n",
       "82                 47.0                 164.0   0.9946  3.17       0.42   \n",
       "83                 42.0                 207.0   0.9966  3.33       0.46   \n",
       "84                 54.0                 155.0   0.9974  3.11       0.45   \n",
       "85                 52.0                 152.0   0.9975  3.12       0.46   \n",
       "86                 55.0                 156.0   0.9974  3.09       0.44   \n",
       "87                 69.0                 202.0   0.9972  3.22       0.48   \n",
       "88                 54.0                 155.0   0.9974  3.11       0.45   \n",
       "89                 52.0                 152.0   0.9975  3.12       0.46   \n",
       "90                 55.0                 156.0   0.9974  3.09       0.44   \n",
       "91                 65.0                 243.0   0.9957  3.12       0.47   \n",
       "92                 47.0                 136.0   0.9900  3.26       0.40   \n",
       "93                 48.0                 136.0   0.9899  3.25       0.41   \n",
       "94                 51.0                 148.0   0.9916  3.21       0.44   \n",
       "95                 62.0                 240.0   0.9969  3.04       0.42   \n",
       "96                 26.0                 164.0   0.9979  3.14       0.50   \n",
       "97                 15.0                  80.0   0.9913  2.95       0.36   \n",
       "98                  4.0                  83.0   0.9956  2.89       0.30   \n",
       "99                 26.0                 164.0   0.9979  3.14       0.50   \n",
       "\n",
       "    alcohol  quality  \n",
       "0       8.8        6  \n",
       "1       9.5        6  \n",
       "2      10.1        6  \n",
       "3       9.9        6  \n",
       "4       9.9        6  \n",
       "5      10.1        6  \n",
       "6       9.6        6  \n",
       "7       8.8        6  \n",
       "8       9.5        6  \n",
       "9      11.0        6  \n",
       "10     12.0        5  \n",
       "11      9.7        5  \n",
       "12     10.8        5  \n",
       "13     12.4        7  \n",
       "14      9.7        5  \n",
       "15     11.4        7  \n",
       "16      9.6        6  \n",
       "17     12.8        8  \n",
       "18     11.3        6  \n",
       "19      9.5        5  \n",
       "20     12.8        8  \n",
       "21     11.0        7  \n",
       "22     10.5        8  \n",
       "23      9.3        5  \n",
       "24     10.0        6  \n",
       "25     10.4        6  \n",
       "26     10.0        6  \n",
       "27     10.5        6  \n",
       "28     11.6        6  \n",
       "29     12.3        7  \n",
       "..      ...      ...  \n",
       "70      9.0        6  \n",
       "71      9.5        5  \n",
       "72     10.0        5  \n",
       "73      9.1        6  \n",
       "74     10.7        8  \n",
       "75      9.5        5  \n",
       "76     12.7        7  \n",
       "77     12.0        7  \n",
       "78      9.3        5  \n",
       "79      9.0        5  \n",
       "80     10.6        6  \n",
       "81      9.2        6  \n",
       "82     10.0        5  \n",
       "83      9.5        6  \n",
       "84      8.7        5  \n",
       "85      8.7        6  \n",
       "86      8.7        6  \n",
       "87      9.7        6  \n",
       "88      8.7        5  \n",
       "89      8.7        6  \n",
       "90      8.7        6  \n",
       "91      9.0        5  \n",
       "92     12.6        7  \n",
       "93     12.6        7  \n",
       "94     11.5        7  \n",
       "95      9.2        6  \n",
       "96      8.8        6  \n",
       "97     11.4        7  \n",
       "98     10.1        4  \n",
       "99      8.8        6  \n",
       "\n",
       "[100 rows x 12 columns]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the regression task data\n",
    "wine_data = pd.read_csv('data/winequality-white.csv', sep=\";\")\n",
    "wine_data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data into X and y data arrays\n",
    "X_regr = wine_data.drop(['quality'], axis=1).values\n",
    "y_regr = wine_data['quality'].values\n",
    "\n",
    "# Standardize the data\n",
    "sc = preprocessing.StandardScaler()\n",
    "sc.fit(X_regr)\n",
    "X_regr = sc.transform(X_regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Cross-validation\n",
    "\n",
    "Let us create a cross-validation utility function (similar to what we have done in Lab 3, but for regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up folds for cross_validation\n",
    "from sklearn import cross_validation\n",
    "folds_regr = cross_validation.KFold(y_regr.size, n_folds=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validate_regr(design_matrix, labels, regressor, cv_folds):\n",
    "    \"\"\" Perform a cross-validation and returns the predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    design_matrix: (n_samples, n_features) np.array\n",
    "        Design matrix for the experiment.\n",
    "    labels: (n_samples, ) np.array\n",
    "        Vector of labels.\n",
    "    regressor:  Regressor instance; must have the following methods:\n",
    "        - fit(X, y) to train the regressor on the data X, y\n",
    "        - predict(X) to apply the trained regressor to the data X and return estimates \n",
    "    cv_folds: sklearn cross-validation object\n",
    "        Cross-validation iterator.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    pred: (n_samples, ) np.array\n",
    "        Vectors of predictions (same order as labels).\n",
    "    \"\"\"\n",
    "    pred = np.zeros(labels.shape)\n",
    "    for tr, te in cv_folds:\n",
    "        regressor.fit(design_matrix[tr,:], labels[tr])\n",
    "        pred[te] = (regressor.predict(design_matrix[te,:]))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Linear regression implementation\n",
    "\n",
    "### Closed-form solution\n",
    "\n",
    "For an input vector $X^T = (X_1 , X_2 , \\dots , X_p )$, and a real-valued output Y, the linear regression model\n",
    "has the form $$f(X) = \\beta_0 + \\sum_{j=1}^pX_j\\beta_j$$\n",
    "We consider a set of training data $(x_1 , y_1 ) \\dots (x_N , y_N )$ from which to estimate the parameters $\\beta$.\n",
    "\n",
    "The most popular estimation method is least squares, in which the coefficients $\\beta = (\\beta_0 , \\beta_1 , \\dots , \\beta_p )^T$ minimize the residual sum of squares $$ RSS(\\beta) = \\sum_{i=1}^N(y_i-f(x_i))^2 = (y-X\\beta)^T(y-X\\beta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is convex.\n",
    "\n",
    "Differentiating with respect to $\\beta$ we obtain $$\\frac{\\partial{RSS}}{\\partial{\\beta}} = -2X^T(y-X\\beta) $$\n",
    "\n",
    "If X^TX is inversible, we obtain a unique solution by setting the first derivative to 0.\n",
    "$$\\hat{\\beta} = (X^TX)^{-1}X^Ty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Remark ** It might happen that the columns of X are not linearly independent (in the case for example of two perfectly correlated inputs). Then $X^TX$ is singular and the least squares coefficients $\\hat{\\beta}$ are not uniquely defined.\n",
    "This can be avoid by dropping redundant columns in X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question:__ Fill in the LeastSquareRegr class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LeastSquaresRegr():\n",
    "    \"\"\" Class for least-squares linear regression:\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "    coef_: 1-dimensional np.array\n",
    "        coefficients of the linear regression (beta)\n",
    "    \"\"\"\n",
    "    def __init__(self,):\n",
    "        self.coef_ = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit the data (X, y).\n",
    "    \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (num_samples, num_features) np.array\n",
    "            Design matrix\n",
    "        y: (num_samples, ) np.array\n",
    "            Output vector\n",
    "        \n",
    "        Note:\n",
    "        -----\n",
    "        Updates self.coef_\n",
    "        \"\"\"\n",
    "        # Create a (num_samples, num_features+1) np.array X_aug whose first column \n",
    "        # is a column of all ones (so as to fit an intercept).\n",
    "        # TODO\n",
    "        (n,p)=X.shape\n",
    "        X_aug=np.ones((n,p+1))\n",
    "        X_aug[:,1:]=X\n",
    "\n",
    "        # Update self.coef_\n",
    "        # TODO\n",
    "        self.coef_=np.dot(np.linalg.inv(X_aug.T.dot(X_aug)),X_aug.T).dot(y)\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" Make predictions for data X.\n",
    "    \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (num_samples, num_features) np.array\n",
    "            Design matrix\n",
    "        \n",
    "        Returns:\n",
    "        -----\n",
    "        y_pred: (num_samples, ) np.array\n",
    "            Predictions\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        (n,p)=X.shape\n",
    "        X_aug=np.ones((n,p+1))\n",
    "        X_aug[:,1:]=X\n",
    "        return self.coef_.dot(X_aug.T)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now evaluate your least squares regression on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.570\n"
     ]
    }
   ],
   "source": [
    "regr = LeastSquaresRegr()\n",
    "pred = cross_validate_regr(X_regr, y_regr, regr, folds_regr)\n",
    "print(\"Mean squared error: %.3f\" % metrics.mean_squared_error(y_regr,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent solution\n",
    "\n",
    "Processing the entire dataset in one go can be computationally costly for large datasets. In addition, a small change in the training set involves computing the new parameters from scratch. In both cases, it may be worthwhile to use sequential algorithms in which the datapoints are considered one at a time and the model parameters updated at each time. \n",
    "\n",
    "We will create a sequential version of our least squares regressor, using gradient descent. \n",
    "\n",
    "__Question:__ Fill in the blanks.\n",
    "\n",
    "__Hints:__ You can use [np.reshape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html) to cast a 1-dimensional np.array of shape (n, ) as a 2-dimensional np.array of shape (n, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class seq_LeastSquaresRegr():\n",
    "    \"\"\" Class for sequential least-squares linear regression:\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "    coef_: 1-dimensional np.array\n",
    "        coefficients of the linear regression (beta)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.coef_ = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit the data (X, y).\n",
    "    \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (num_samples, num_features) np.array\n",
    "            Design matrix\n",
    "        y: (num_sampes, ) np.array\n",
    "            Output vector\n",
    "        \n",
    "        Note:\n",
    "        -----\n",
    "        Updates self.coef_\n",
    "        \"\"\"\n",
    "        # Create a (num_samples, num_features+1) np.array X_aug whose first column \n",
    "        # is a column of all ones (for beta_0)\n",
    "        # TODO\n",
    "        X_aug= np.ones((X.shape[0],X.shape[1]+1))\n",
    "        X_aug[:,1:]=X\n",
    "        # Initialize self.coef_ at random, with the right shape\n",
    "        # TODO\n",
    "        self.coef_=np.random.randn(X_aug.shape[1],)\n",
    "         \n",
    "        def f_ls(beta):\n",
    "            \"\"\" Returns the least square error between y and X.beta.        \n",
    "            \"\"\"\n",
    "            # TODO\n",
    "            phi=np.squeeze(np.dot(X_aug,beta.reshape([-1,1])))\n",
    "            return np.mean(0.5*(y-phi)**2)\n",
    "\n",
    "        \n",
    "        def fprime_ls(beta):\n",
    "            \"\"\" Returns the gradient of f_ls at beta.\n",
    "            IMPORTANT:  The output should have the same shape as beta,\n",
    "            otherwise our optimiser will not work.\n",
    "            \"\"\"\n",
    "            # TODO\n",
    "            phi=np.dot(X_aug,beta.reshape([-1,1]))\n",
    "            diff=phi-y.reshape([-1,1])\n",
    "            diff=np.tile(diff,[1,X_aug.shape[1]])\n",
    "            return np.mean(X_aug*diff,axis=0)\n",
    "        \n",
    "        # Use gradient descent optimization to minimize the least squares error\n",
    "        self.gd = GradientDescentOptimizer(f_ls, fprime_ls, self.coef_, lr=1e-2)\n",
    "        self.gd.optimize(max_iter=1000)\n",
    "        \n",
    "        # Update self.coef_\n",
    "        # TODO\n",
    "        self.coef_=self.gd.beta\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\" Make predictions for data X.\n",
    "    \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (num_samples, num_features) np.array\n",
    "            Design matrix\n",
    "        \n",
    "        Returns:\n",
    "        -----\n",
    "        y_pred: (num_samples, ) np.array\n",
    "            Predictions\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        X_aug=np.ones((X.shape[0],X.shape[1]+1))\n",
    "        X_aug[:,1:]=X\n",
    "        return np.squeeze(np.dot(X_aug,self.coef_.reshape([-1,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((5L,), array([1, 2, 3, 4, 5]))\n",
      "((5L, 1L), array([[1],\n",
      "       [2],\n",
      "       [3],\n",
      "       [4],\n",
      "       [5]]))\n",
      "((5L,), array([1, 2, 3, 4, 5]))\n",
      "(5L, 7L) [[1 1 1 1 1 1 1]\n",
      " [2 2 2 2 2 2 2]\n",
      " [3 3 3 3 3 3 3]\n",
      " [4 4 4 4 4 4 4]\n",
      " [5 5 5 5 5 5 5]]\n"
     ]
    }
   ],
   "source": [
    "x=np.array([1,2,3,4,5])\n",
    "print (x.shape,x)\n",
    "x2=x.reshape([-1,1])\n",
    "print (x2.shape,x2)\n",
    "x3=np.squeeze(x2)\n",
    "print (x3.shape,x3)\n",
    "\n",
    "M=np.tile(x2,[1,7])\n",
    "print M.shape,M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now evaluate your sequential least squares regression on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.583\n"
     ]
    }
   ],
   "source": [
    "regr = seq_LeastSquaresRegr()\n",
    "pred = cross_validate_regr(X_regr, y_regr, regr, folds_regr)\n",
    "print(\"Mean squared error: %.3f\" % metrics.mean_squared_error(y_regr, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question ** Discuss the difference of errors between the two implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__ The first error is lower because it's the analytical solution, which is more accurate than the gradient solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### scikit-learn implementation\n",
    "\n",
    "We will now compare to the scikit-learn implementation.\n",
    "\n",
    "__Question__ Cross-validate scikit-learn's [linear_model.LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.570\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# Initialize a LinearRegression model\n",
    "regr = linear_model.LinearRegression() # TODO\n",
    "\n",
    "# Cross-validate it\n",
    "pred =cross_validate_regr(X_regr, y_regr, regr, folds_regr)  # TODO\n",
    "print(\"Mean squared error: %.3f\" % metrics.mean_squared_error(y_regr, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: 0.586\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "pred =cross_validate_regr(X_regr, y_regr, regr, folds_regr)\n",
    "print(\"Mean absolute error: %.3f\" % metrics.mean_absolute_error(y_regr, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Logistic regression\n",
    "\n",
    "We will now implement a linear regression, first using the closed form solution, and second with our gradient descent.\n",
    "\n",
    "## 2.1 Logistic regression data\n",
    "\n",
    "Our second data set comes from the world of bioinformatics. In this data set, each observation is a tumor, and it is described by the expression of 3,000 genes. The expression of a gene is a measure of how much of that gene is present in the biological sample. Because this affects how much of the protein this gene codes for is produced, and because proteins dictacte what cells can do, gene expression gives us valuable information about the tumor. In particular, the expression of the same gene in the same individual is different in different tissues (although the DNA is the same): this is why blood cells look different from skin cells. In our data set, there are two types of tumors: breast tumors and ovary tumors. Let us see if gene expression can be used to separate them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(542L, 3000L)\n"
     ]
    }
   ],
   "source": [
    "# Load the classification task data\n",
    "breast_data = pd.read_csv('data/small_Breast_Ovary.csv')\n",
    "\n",
    "\n",
    "# Drop the 'Tissue' column to create the design matrix\n",
    "X_clf = np.array(breast_data.drop(['Tissue', 'ID_REF'], axis=1).values)\n",
    "print X_clf.shape #n=542 p=3000\n",
    "\n",
    "\n",
    "# Use the 'Tissue' column to create the labels (0=Breast, 1=Ovary)\n",
    "y_clf = np.array(breast_data['Tissue'].values)\n",
    "y_clf[np.where(y_clf == 'Breast')] = 0\n",
    "y_clf[np.where(y_clf == 'Ovary')] = 1\n",
    "y_clf = y_clf.astype(np.int)\n",
    "\n",
    "#sc = preprocessing.StandardScaler()\n",
    "#sc.fit(X_clf)\n",
    "#X_clf = sc.transform(X_clf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question:__ How many samples do we have? How many belong to each class? How many features do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(542L, 3000L)\n",
      "Breast is: 0\n",
      "Ovary is:  0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "print X_clf.shape #samples=542 #features=3000\n",
    "#y_clf = np.array(breast_data['Tissue'].values)\n",
    "\n",
    "i=0\n",
    "j=0\n",
    "for tissue in y_clf:\n",
    "    if tissue=='Breast':\n",
    "        i=i+1\n",
    "    if tissue=='Ovary':\n",
    "        j=j+1\n",
    "print 'Breast is:', i\n",
    "print 'Ovary is: ', j\n",
    "print i+j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Cross-validation\n",
    "\n",
    "Let us create a cross-validation utility function (similar to what we have done in Lab 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up folds for cross_validation\n",
    "from sklearn import cross_validation\n",
    "folds_clf = cross_validation.StratifiedKFold(y_clf, n_folds=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validate_clf(design_matrix, labels, classifier, cv_folds):\n",
    "    \"\"\" Perform a cross-validation and returns the predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    design_matrix: (n_samples, n_features) np.array\n",
    "        Design matrix for the experiment.\n",
    "    labels: (n_samples, ) np.array\n",
    "        Vector of labels.\n",
    "    classifier:  sklearn classifier object\n",
    "        Classifier instance; must have the following methods:\n",
    "        - fit(X, y) to train the classifier on the data X, y\n",
    "        - predict_proba(X) to apply the trained classifier to the data X and return probability estimates \n",
    "    cv_folds: sklearn cross-validation object\n",
    "        Cross-validation iterator.\n",
    "        \n",
    "    Return:\n",
    "    -------\n",
    "    pred: (n_samples, ) np.array\n",
    "        Vectors of predictions (same order as labels).\n",
    "    \"\"\"\n",
    "    pred = np.zeros(labels.shape)\n",
    "    for tr, te in cv_folds:\n",
    "        classifier.fit(design_matrix[tr,:], labels[tr])\n",
    "        pred[te] = classifier.predict_proba(design_matrix[te,:])[:,1]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Logistic regression implementation\n",
    "\n",
    "Under the assumption of linear boundaries segragating classes, the posterior probability of class $C_1$ can be written as a logistic sigmoid acting on a linear function of the feature vector X so that $p(C_1|X) = \\sigma(\\beta^TX)$.\n",
    "\n",
    "For an input vector $X^T = (X_1 , X_2 , \\dots , X_p )$, and a binary output Y such that $y_i\\in\\{0,1\\}$, the likelihood function is\n",
    "$$ p(y\\vert\\beta) = \\prod_{i=0}^Np(y_i\\vert\\beta) = \\prod_{i=0}^Np(C_1\\vert X)^{y_i}(1-p(C_1\\vert X))^{1-y_i} = \\prod_{i=0}^N\\sigma(\\beta^TX)^{y_i}(1-\\sigma(\\beta^TX))^{1-y_i}.$$\n",
    "\n",
    "We can then define (as usual) an error function to minimize by taking the negative logarithm of the likelihood\n",
    "$$ E_{loss}(\\beta) = -\\log(p(y|\\beta)) = -\\sum_{i=0}^N\\{ y_i\\log(\\sigma(\\beta^Tx_i)) + (1-y_i)\\log(1-\\sigma(\\beta^Tx_i)) \\} = \\sum_{i=0}^N\\log(e^{-y_i*x_i^T\\beta}+1)$$\n",
    "\n",
    "Then it's derivative can be written $$\\nabla E(\\beta) = \\sum_{i=1}^N(\\sigma(x_i^T\\beta)-y_i)x_i.$$\n",
    "\n",
    "__Question:__ Now that you know how to write the loss function for logistic regression, `f(beta)`, and what its derivative `f'(beta)` is in terms of `beta`, use the previously defined `GradientDescentOptimizer` to fit a logistic regression model to our data `(X, y)`. Some of the code below has already been filled to help you. \n",
    "\n",
    "**Remark:** The derivative of logistic function $\\sigma(x)=\\frac{1}{1+e^{-x}}$ can be written $\\text{d}\\sigma = \\sigma (1-\\sigma)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "from sklearn import metrics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\" Sigmoid function.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x: np.array\n",
    "        input variables\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    r: np.array of same dimension as x\n",
    "        outputs\n",
    "    \"\"\"\n",
    "    # Truncate the very large and very small values of x to avoid overflowing the exponential\n",
    "    x[np.where(x > 7e2)] = 7e2\n",
    "    x[np.where(x < -7e2)] = -7e2\n",
    "    \n",
    "    # Compute the sigmoid\n",
    "    r = 1. / (1 + np.exp(-1*x))\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class LogisticRegr():\n",
    "    \"\"\" Class for sequential least-squares linear regression:\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "    coef_: 1-dimensional np.array\n",
    "        coefficients of the linear regression (beta)\n",
    "    classes: list\n",
    "        list of class labels\n",
    "    \"\"\"\n",
    "    def __init__(self,):\n",
    "        self.coef_ = None\n",
    "        self.classes_ = [0,1]\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit the data (X, y).\n",
    "    \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (num_samples, num_features) np.array\n",
    "            Design matrix\n",
    "        y: (num_sampes, ) np.array\n",
    "            Output vector\n",
    "        \n",
    "        Note:\n",
    "        -----\n",
    "        Updates self.coef_\n",
    "        \"\"\"\n",
    "        # Create a (num_samples, num_features+1) np.array X_aug whose first column \n",
    "        #    is a column of all ones (for beta_0), while the remaining columns are \n",
    "        #    taken from X.\n",
    "        # TODO\n",
    "        X_aug=np.ones((X.shape[0], X.shape[1]+1))\n",
    "        X_aug[:,1:]=X\n",
    "        # Initialize self.coef_ at random, with the right shape\n",
    "        # TODO\n",
    "        #self.coef_=np.random.randn(X_aug.shape[1],)\n",
    "        self.coef_=np.random.normal(size=(X.shape[1]+1,), loc=0.0, scale=0.1)\n",
    "        # Find the indices which correspond to class 0\n",
    "        c0_ids = np.where(y == 0)[0] \n",
    "        # Find the indices which correspond to class 1. \n",
    "        c1_ids = np.where(y == 1)[0]\n",
    "        # TODO\n",
    "            \n",
    "        def f_lr(beta):            \n",
    "            \"\"\" \n",
    "            Returns the logistic loss between y and X.beta.        \n",
    "            \"\"\"\n",
    "            # Reshape beta as a two-dimensional array with second dimension equal to 1\n",
    "            beta  = np.reshape(beta,[-1,1])# TODO\n",
    "            \n",
    "            # Compute phi_0, the model's prediction for class 0 samples\n",
    "            phi_0 = sigmoid(np.dot(X_aug[c0_ids,:], beta))# TODO\n",
    "            \n",
    "            # Compute phi_1, the model's prediction for class 1 samples\n",
    "            phi_1 = sigmoid(np.dot(X_aug[c1_ids,:], beta))# TODO\n",
    "            \n",
    "            # Compute the loss over class 1 samples\n",
    "            loss = -1*np.sum(np.log(phi_1))# TODO\n",
    "            # Update the loss with the loss over class 0 samples\n",
    "            loss = loss - np.sum(np.log(1-phi_0))# TODO\n",
    "            return loss\n",
    "        \n",
    "        def fprime_lr(beta):   \n",
    "            \"\"\" Returns the gradient of f_ls at beta.\n",
    "            IMPORTANT:  The output should have the same shape as beta,\n",
    "            otherwise our optimiser will not work!\n",
    "            \"\"\"\n",
    "            # TODO\n",
    "            phi = sigmoid(np.dot(X_aug, beta.reshape([-1,1])))\n",
    "            diff = phi - y.reshape([-1,1])\n",
    "            grad = X_aug*diff\n",
    "            return np.mean(grad,axis=0)\n",
    "        \n",
    "        # Now optimize the loss over the data:\n",
    "        # Use GradientDescentOptmizer to update self.coef_\n",
    "        # TODO\n",
    "        \n",
    "        self.gd = GradientDescentOptimizer(f_lr, fprime_lr, self.coef_, lr=2e-2)\n",
    "        self.gd.optimize(max_iter=100)\n",
    "        self.coef_=self.gd.beta\n",
    " \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\" Make probabilistic predictions for data X.\n",
    "    \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (num_samples, num_features) np.array\n",
    "            Design matrix\n",
    "        \n",
    "        Returns:\n",
    "        -----\n",
    "        y_pred: (num_samples, ) np.array\n",
    "            Predictions (probabilities of belonging to class 1)\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        (n,p)=X.shape\n",
    "        X_aug=np.ones((n,p+1))\n",
    "        X_aug[:,1:]=X\n",
    "        r=sigmoid(np.dot(X_aug,self.coef_.reshape([-1,1]))).squeeze()\n",
    "        pred=np.zeros((n,2))\n",
    "        pred[:,1]=r\n",
    "        pred[:,0]=1-r\n",
    "        return pred\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" Make binary predictions for data X.\n",
    "    \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (num_samples, num_features) np.array\n",
    "            Design matrix\n",
    "        \n",
    "        Returns:\n",
    "        -----\n",
    "        y_pred: (num_samples, ) np.array\n",
    "            Predictions (0 or 1)\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        pred=self.predict_proba(X)\n",
    "        return pred.argmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now evaluate your logistic regression on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.919\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegr()\n",
    "pred = cross_validate_clf(X_clf, y_clf, clf, folds_clf)\n",
    "print \n",
    "print(\"Accuracy: %.3f\" % metrics.accuracy_score(y_clf, pred > 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### scikit-learn implementation\n",
    "\n",
    "We will now compare to the scikit-learn implementation.\n",
    "\n",
    "__Question__ Cross-validate scikit-learn's [linear_model.LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.961\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# Initialize a LogisticRegression model. \n",
    "# Use C=1e7 to ensure there is no regularization (we'll talk about regularization next time!)\n",
    "clf = linear_model.LogisticRegression(C=1e7) # TODO\n",
    "\n",
    "# Cross-validate it\n",
    "ypred_logreg = cross_validate_clf(X_clf, y_clf, clf, folds_clf)# TODO\n",
    "a = np.where(ypred_logreg > 0.5, 1, 0)\n",
    "print(\"Accuracy: %.3f\" % metrics.accuracy_score(y_clf,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334.0\n",
      "10.0\n",
      "0.970930232558\n",
      "187.0\n",
      "11.0\n",
      "0.944444444444\n",
      "Recall: 0.944\n",
      "Recall: 0.971\n"
     ]
    }
   ],
   "source": [
    "TP=0\n",
    "FN=0\n",
    "for i in range(len(y_clf)):\n",
    "    if y_clf[i]==0 and a[i]==0:\n",
    "        TP+=1\n",
    "    if y_clf[i]==0 and a[i]==1:\n",
    "        FN+=1\n",
    "TP1=0\n",
    "FN1=0\n",
    "for i in range(len(y_clf)):\n",
    "    if y_clf[i]==1 and a[i]==1:\n",
    "        TP1+=1\n",
    "    if y_clf[i]==1 and a[i]==0:\n",
    "        FN1+=1      \n",
    "\n",
    "TP=float(TP)\n",
    "FN=float(FN)\n",
    "TP1=float(TP1)\n",
    "FN1=float(FN1)\n",
    "\n",
    "print TP\n",
    "print FN\n",
    "recall=(TP)/(TP+FN) \n",
    "print recall\n",
    "\n",
    "print TP1\n",
    "print FN1\n",
    "recall1=(TP1)/(TP1+FN1)\n",
    "print recall1\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "print(\"Recall: %.3f\" % metrics.recall_score(y_clf,a))\n",
    "print(\"Recall: %.3f\" % metrics.recall_score(y_clf,a,labels=None,pos_label=0,average=\"binary\",sample_weight=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334.0\n",
      "10.0\n",
      "0.970930232558\n",
      "Recall: 0.971\n"
     ]
    }
   ],
   "source": [
    "#my way\n",
    "TP=0\n",
    "FN=0\n",
    "for i in range(len(y_clf)):\n",
    "    if y_clf[i]==0 and a[i]==0:\n",
    "        TP+=1\n",
    "    if y_clf[i]==0 and a[i]==1:\n",
    "        FN+=1\n",
    "\n",
    "TP=float(TP)\n",
    "FN=float(FN)\n",
    "print TP\n",
    "print FN\n",
    "recall=(TP)/(TP+FN) \n",
    "print recall\n",
    "\n",
    "#skitlearn\n",
    "print(\"Recall: %.3f\" % metrics.recall_score(y_clf,a,labels=None,pos_label=0,average=\"binary\",sample_weight=None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question : ** Plot the ROC curve. Use plt.semilogx to use a logarithmic scale on the x-axis. This \"spreads out\" the curve a little, making it easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x12d8a780>"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEhCAYAAABhpec9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15fe5b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr_logreg, tpr_logreg, thresholds = metrics.roc_curve(y_clf, ypred_logreg, pos_label=1) # TODO\n",
    "auc_logreg =metrics.auc(fpr_logreg, tpr_logreg) # TODO\n",
    "\n",
    "plt.semilogx(fpr_logreg, tpr_logreg, '-', color='orange', \n",
    "             label='AUC = %0.3f' % auc_logreg)\n",
    "\n",
    "#plt.plot(fpr_logreg, tpr_logreg, '-', color='orange', \n",
    "             #label='AUC = %0.3f' % auc_logreg)\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.title('ROC curve: Logistic regression', fontsize=16)\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data scaling\n",
    "See [preprocessing.StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Scale the data, and compute the cross-validated predictions of the logistic regression on the scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.958\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Scale the data with preprocessing.StandardScaler\n",
    "# Initialize a scaler\n",
    "scaler =preprocessing.StandardScaler(copy=True,with_mean=True, with_std=True) # TODO\n",
    "# Scale your design matrix\n",
    "X_clf_scaled =preprocessing.scale(X_clf) # TODO\n",
    "\n",
    "# Initialize a LogisticRegression model. \n",
    "# Use C=1e7 to ensure there is no regularization (we'll talk about regularization next time!)\n",
    "clf =linear_model.LogisticRegression(C=1e7) # TODO\n",
    "\n",
    "# Cross-validate it for the scaled data\n",
    "ypred_logreg_scaled = cross_validate_clf(X_clf_scaled,y_clf,clf,folds_clf)# TODO\n",
    "a=np.where(ypred_logreg_scaled>0.5,1,0)\n",
    "\n",
    "print(\"Accuracy: %.3f\" % metrics.accuracy_score(y_clf,a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Plot the two ROC curves (one for the logistic regression on the original data, one for the logistic regression on the scaled data) on the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x13364e80>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEhCAYAAABhpec9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAwP2I5XSw/CbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118952b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr_logreg_scaled, tpr_logreg_scaled, thresholds =metrics.roc_curve(y_clf,ypred_logreg_scaled,pos_label=1) # TODO\n",
    "auc_logreg_scaled = metrics.auc(fpr_logreg_scaled,tpr_logreg_scaled) # TODO\n",
    "\n",
    "plt.semilogx(fpr_logreg_scaled, tpr_logreg_scaled, '-', color='blue', \n",
    "             label='scaled data; AUC = %0.3f' % auc_logreg_scaled)\n",
    "plt.semilogx(fpr_logreg, tpr_logreg, '-', color='orange', \n",
    "             label='original data; AUC = %0.3f' % auc_logreg)\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.title('ROC curve: Logistic regression', fontsize=16)\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a cross-validation setting, we ignore the samples from the test fold when training the classifier. This also means that scaling should be done on the training data only. \n",
    "\n",
    "In scikit-learn, we can use a scaler to make centering and scaling happen independently on each feature by computing the relevant statistics on the samples *in the training set*. \n",
    "The mean and standard deviation will be stored to be used on the test data.\n",
    "\n",
    "**Question** Rewrite the cross_validate method to include a scaling step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validate_clf_with_scaling(design_matrix, labels, classifier, cv_folds):\n",
    "    \"\"\" Perform a cross-validation and returns the predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    design_matrix: (n_samples, n_features) np.array\n",
    "        Design matrix for the experiment.\n",
    "    labels: (n_samples, ) np.array\n",
    "        Vector of labels.\n",
    "    classifier:  sklearn classifier object\n",
    "        Classifier instance; must have the following methods:\n",
    "        - fit(X, y) to train the classifier on the data X, y\n",
    "        - predict_proba(X) to apply the trained classifier to the data X and return probability estimates \n",
    "    cv_folds: sklearn cross-validation object\n",
    "        Cross-validation iterator.\n",
    "        \n",
    "    Return:\n",
    "    -------\n",
    "    pred: (n_samples, ) np.array\n",
    "        Vectors of predictions (same order as labels).\n",
    "    \"\"\"\n",
    "    pred = np.zeros(labels.shape)\n",
    "    for tr, te in cv_folds:\n",
    "        # TODO\n",
    "        preprocessing.scale(design_matrix[tr,:])\n",
    "        classifier.fit(design_matrix[tr,:],labels[tr])\n",
    "        pred[te]=classifier.predict_proba(design_matrix[te,:])[:,1]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Now use the cross_validate_with_scaling method to cross-validate the logistic regression on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.946494464945\n"
     ]
    }
   ],
   "source": [
    "clf = linear_model.LogisticRegression(C=1e6) \n",
    "ypred_logreg_scaled_ = cross_validate_clf_with_scaling(X_clf, y_clf, clf, folds_clf)\n",
    "print(metrics.accuracy_score(y_clf,np.where(ypred_logreg_scaled_ > 0.5, 1, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Again, compare the AUROC and ROC curves with those obtained previously. What do you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x131bea20>"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEhCAYAAABhpec9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkSIFChERKVKgEBGRoh9ggN8rRvGqFAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12b48da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr_logreg_scaled, tpr_logreg_scaled, thresholds =metrics.roc_curve(y_clf,ypred_logreg_scaled,pos_label=1) # TODO\n",
    "auc_logreg_scaled = metrics.auc(fpr_logreg_scaled,tpr_logreg_scaled)\n",
    "\n",
    "plt.semilogx(fpr_logreg_scaled, tpr_logreg_scaled, '-', \n",
    "             color='blue', label='scaled data overfit; AUC = %0.3f' % auc_logreg_scaled)\n",
    "plt.semilogx(fpr_logreg, tpr_logreg, '-', color='orange', \n",
    "             label='original data; AUC = %0.3f' % auc_logreg)\n",
    "plt.semilogx(fpr_logreg_scaled, tpr_logreg_scaled, '-', color='black', \n",
    "             label='scaled data no overfit; AUC = %0.3f' % auc_logreg_scaled)\n",
    "\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.title('ROC curve: Logistic regression', fontsize=16)\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Kaggle challenge ideas\n",
    "\n",
    "* Load the data\n",
    "* Set up a cross-validation that you will use for all your evaluations. Notice there is a 'random_state' parameter to the cross-validation methods of scikit-learn, that you can use to ensure you always get the same splits.\n",
    "* To go one step further in ensuring a fair comparison of your algorithms, you can use multiple repeats of the cross-validation procedure (using different splits each time), and report the mean & standard deviation over the repeats of the performance obtained. If you do this, you can report standard deviations in plots by using error bars.\n",
    "* Evaluate the performance of a linear regression on your data. Which evaluation metric are you using? See http://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics for help defining one.\n",
    "* Submit a linear regression predictor to the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
